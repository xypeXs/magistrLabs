{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7a38d6-a09d-4f64-9b9a-cd5399b611be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in e:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in e:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in e:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2025.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02cc5c1-9747-4b65-924a-8e1d78449e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21dd072e-3717-4022-8589-838e33052af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = 'datasets/IMDB_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9f1fe7-6e96-4f92-9b27-4086212ff36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(imdb_path)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28be9a23-559f-4aab-b456-121dfa33c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'as', 'them', 'up', 'do', 'aren', 'same', 't', 'through', 'had', 'isn', 'itself', 'nor', \"you're\", 'yourself', \"won't\", 'i', 'when', 'whom', 'the', 'd', \"hasn't\", 'too', \"they've\", 'at', \"i'd\", 'few', 'her', 'over', \"wasn't\", 'wouldn', 'y', 'during', 'most', 'just', 'from', 'doesn', 'he', 'you', 'should', 'own', 'for', 'those', 'does', 'o', \"hadn't\", 'am', 've', \"it'll\", \"wouldn't\", 's', 'while', 'there', 'did', \"didn't\", 'more', 'then', 'don', 'has', 'who', \"i'm\", 'no', 'what', 'doing', 'or', 'where', 'which', 'won', 'shouldn', 'but', \"needn't\", 'himself', 'being', 'were', 'your', \"don't\", \"haven't\", \"it's\", 'that', \"you'll\", 'both', \"isn't\", 'be', \"we'd\", 'so', 'very', 'couldn', 'their', 'between', 'can', \"weren't\", 'weren', \"we'll\", 'once', 'here', 'this', 'yours', \"that'll\", \"you'd\", 'our', 'his', 'some', \"we've\", 'ours', 'if', \"aren't\", 'into', 'ourselves', 'all', 'm', \"couldn't\", \"he'd\", 'we', 'above', \"you've\", \"i've\", 'these', 'ma', \"doesn't\", 'of', 'now', \"they're\", \"she's\", 'will', 'have', 'to', 'in', 'hers', 'll', 'on', 'him', 'before', 're', 'such', 'wasn', 'only', 'been', 'my', 'was', 'themselves', 'they', 'its', \"she'll\", \"we're\", 'didn', 'each', 'why', \"should've\", \"she'd\", 'below', 'needn', 'not', 'other', 'than', 'myself', 'theirs', 'ain', \"it'd\", 'off', 'herself', \"he'll\", 'she', 'and', 'yourselves', 'about', 'any', 'is', 'haven', 'a', 'an', \"shan't\", 'because', \"they'd\", \"they'll\", 'with', 'how', 'until', 'out', 'shan', 'hadn', 'it', 'further', 'against', \"he's\", 'having', 'again', 'are', 'me', 'mightn', 'after', 'down', \"mightn't\", 'mustn', \"shouldn't\", 'under', 'hasn', 'by', \"i'll\", \"mustn't\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9840234f-bc51-4424-904d-013180ed375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Local\\Temp\\ipykernel_2212\\1900297942.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_data = y_data.replace('negative', 0)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv(imdb_path)\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "\n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50daa836-9cf8-4f0b-946f-9c8f43913985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "40244    [significant, spoilers, this, sick, disturbing...\n",
      "3205     [i, excited, hyped, watching, film, promos, fi...\n",
      "38579    [this, film, released, year, i, born, like, i,...\n",
      "35314    [two, years, ago, berlin, film, festival, watc...\n",
      "23174    [having, seen, full, length, film, kieslowski,...\n",
      "                               ...                        \n",
      "15768    [alice, kind, movie, made, never, attempts, ev...\n",
      "32163    [pet, sematary, good, horror, film, believe, s...\n",
      "13057    [i, sophomore, college, movie, came, i, never,...\n",
      "30972    [this, movie, somewhat, based, exit, rob, half...\n",
      "11186    [the, positive, reviews, page, planted, filmma...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "13627    [an, unusual, revisionist, western, well, wort...\n",
      "31765    [this, movie, blows, feet, this, debut, movie,...\n",
      "47660    [the, best, scene, the, people, across, the, l...\n",
      "36459    [i, luxury, seeing, movie, i, rather, young, m...\n",
      "34913    [john, huston, actor, director, better, known,...\n",
      "                               ...                        \n",
      "41523    [this, movie, tired, yet, weirdly, childish, c...\n",
      "21109    [i, saw, movie, friend, couldnt, stop, laughin...\n",
      "31707    [biographical, tale, life, charles, lindbergh,...\n",
      "18086    [i, years, old, movie, premiered, television, ...\n",
      "30622    [i, seen, movie, years, i, know, i, would, lik...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "40244    0\n",
      "3205     0\n",
      "38579    1\n",
      "35314    0\n",
      "23174    1\n",
      "        ..\n",
      "15768    1\n",
      "32163    1\n",
      "13057    0\n",
      "30972    0\n",
      "11186    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "13627    1\n",
      "31765    1\n",
      "47660    0\n",
      "36459    0\n",
      "34913    1\n",
      "        ..\n",
      "41523    0\n",
      "21109    1\n",
      "31707    1\n",
      "18086    1\n",
      "30622    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21224451-8701-49fc-a907-ad4cafb1ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06aa9668-8036-48fc-a7ef-afd47067c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[ 2659   983     8 ... 20121   480  9495]\n",
      " [    1  2267  5870 ...   662     1    31]\n",
      " [    8     4   523 ...  2948 10634  1296]\n",
      " ...\n",
      " [    1 15897  1013 ...  4389   557     6]\n",
      " [    8     3   569 ...     3     9   330]\n",
      " [    2  1073   750 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  691  1747 17768 ...     0     0     0]\n",
      " [    8     3  3224 ...     0     0     0]\n",
      " [    2    45    57 ... 17468 21011  5349]\n",
      " ...\n",
      " [12277   695    42 ...   189    12  1977]\n",
      " [    1    71    70 ...     0     0     0]\n",
      " [    1    38     3 ...     1   167     5]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b236cf90-d5fb-483d-bea5-940044dcbf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 130, 32)           2957664   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,982,561\n",
      "Trainable params: 2,982,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd863a4c-413a-4d12-b6f1-1e5dc4769b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "models_path = 'models'\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    models_path,\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66ebe745-1132-4d42-abad-73e938dc2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9617\n",
      "Epoch 1: accuracy improved from -inf to 0.96167, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.1200 - accuracy: 0.9617\n",
      "Epoch 2/10\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9789\n",
      "Epoch 2: accuracy improved from 0.96167 to 0.97887, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0757 - accuracy: 0.9789\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9860\n",
      "Epoch 3: accuracy improved from 0.97887 to 0.98600, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0525 - accuracy: 0.9860\n",
      "Epoch 4/10\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9882\n",
      "Epoch 4: accuracy improved from 0.98600 to 0.98820, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0458 - accuracy: 0.9882\n",
      "Epoch 5/10\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9891\n",
      "Epoch 5: accuracy improved from 0.98820 to 0.98913, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 30ms/step - loss: 0.0425 - accuracy: 0.9891\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9906\n",
      "Epoch 6: accuracy improved from 0.98913 to 0.99063, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0364 - accuracy: 0.9906\n",
      "Epoch 7/10\n",
      "311/313 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9941\n",
      "Epoch 7: accuracy improved from 0.99063 to 0.99415, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0251 - accuracy: 0.9941\n",
      "Epoch 8/10\n",
      "310/313 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9927\n",
      "Epoch 8: accuracy did not improve from 0.99415\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0301 - accuracy: 0.9927\n",
      "Epoch 9/10\n",
      "310/313 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9952\n",
      "Epoch 9: accuracy improved from 0.99415 to 0.99520, saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0217 - accuracy: 0.9952\n",
      "Epoch 10/10\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9919\n",
      "Epoch 10: accuracy did not improve from 0.99520\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0353 - accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14276b2b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a0371c-d1f0-440c-919a-354742f734e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 8ms/step\n",
      "Correct Prediction: 4984\n",
      "Wrong Prediction: 5016\n",
      "Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(x_test, batch_size=128)\n",
    "\n",
    "# Для многоклассовой классификации - берем класс с максимальной вероятностью\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Если y_test тоже в one-hot encoding, преобразуем его\n",
    "if y_test.ndim > 1:\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_test_labels = y_test\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test_labels):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "092a35e9-68af-49d7-adb6-8a754b4e4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f7ab00b-e301-46c9-ae5e-23875e249c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Movie Review:  it's amazing, googd film\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35eae122-bed2-4bf2-ada6-ef2b6466aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  its amazing googd film\n",
      "Filtered:  ['amazing googd film']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57555c3e-fe88-4488-ab5d-31764e256c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[396   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3181e7be-bd26-4dda-800a-677047a6c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 299ms/step\n",
      "[[0.9993506]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd261382-272f-488a-a320-08f189359c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a7b343d-6788-4eaa-b396-bf33d7b6b708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU available: {gpus[0]}\")\n",
    "    # Вывод информации о GPU\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"{details.get('device_name', 'Unknown GPU')}\")\n",
    "else:\n",
    "    print(\"GPU doesn\\t available, CPU using\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86488b8-fa38-41f3-ab31-45e6395c40f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Загружено текстов: 10000\n",
      "\n",
      "1. symbols gen\n",
      "texts: 1000\n",
      "unique symbols: 46\n",
      "sequences count: 1224915\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "training symbols model\n",
      "Epoch 1/15\n",
      "4785/4785 [==============================] - 397s 81ms/step - loss: 1.8976 - accuracy: 0.4371\n",
      "Epoch 2/15\n",
      "4785/4785 [==============================] - 387s 81ms/step - loss: 1.6615 - accuracy: 0.4984\n",
      "Epoch 3/15\n",
      "4785/4785 [==============================] - 371s 78ms/step - loss: 1.6040 - accuracy: 0.5145\n",
      "Epoch 4/15\n",
      "4785/4785 [==============================] - 390s 81ms/step - loss: 1.5746 - accuracy: 0.5229\n",
      "Epoch 5/15\n",
      "4785/4785 [==============================] - 371s 78ms/step - loss: 1.5546 - accuracy: 0.5284\n",
      "Epoch 6/15\n",
      "4785/4785 [==============================] - 353s 74ms/step - loss: 1.5413 - accuracy: 0.5322\n",
      "Epoch 7/15\n",
      "4785/4785 [==============================] - 350s 73ms/step - loss: 1.5306 - accuracy: 0.5349\n",
      "Epoch 8/15\n",
      "4785/4785 [==============================] - 360s 75ms/step - loss: 1.5219 - accuracy: 0.5372\n",
      "Epoch 9/15\n",
      "4785/4785 [==============================] - 378s 79ms/step - loss: 1.5168 - accuracy: 0.5382\n",
      "Epoch 10/15\n",
      "4785/4785 [==============================] - 370s 77ms/step - loss: 1.5108 - accuracy: 0.5397\n",
      "Epoch 11/15\n",
      "4785/4785 [==============================] - 390s 82ms/step - loss: 1.5069 - accuracy: 0.5414\n",
      "Epoch 12/15\n",
      "4785/4785 [==============================] - 350s 73ms/step - loss: 1.5019 - accuracy: 0.5422\n",
      "Epoch 13/15\n",
      "4785/4785 [==============================] - 355s 74ms/step - loss: 1.4988 - accuracy: 0.5429\n",
      "Epoch 14/15\n",
      "4785/4785 [==============================] - 359s 75ms/step - loss: 1.4952 - accuracy: 0.5436\n",
      "Epoch 15/15\n",
      "4785/4785 [==============================] - 360s 75ms/step - loss: 1.4927 - accuracy: 0.5442\n",
      "testing symbols gen\n",
      "'the movie' -> 'the movie is unlik ever the night on to a personals used by'\n",
      "'this film' -> 'this film rane's stroughout any to be deeply films passion '\n",
      "'i think' -> 'i thinking ? but caption heavid is a movie take drams muc'\n",
      "'the story' -> 'the story time ? guy the interest is working quite people v'\n",
      "\n",
      "2. words gen\n",
      "texts: 1000\n",
      "word sequences: 206586\n",
      "dictionary size: 11622\n",
      "input sequence length: 2\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "training words model\n",
      "Epoch 1/15\n",
      "807/807 [==============================] - 54s 59ms/step - loss: 6.6795 - accuracy: 0.0802\n",
      "Epoch 2/15\n",
      "807/807 [==============================] - 48s 59ms/step - loss: 6.1455 - accuracy: 0.1143\n",
      "Epoch 3/15\n",
      "807/807 [==============================] - 48s 60ms/step - loss: 5.9374 - accuracy: 0.1263\n",
      "Epoch 4/15\n",
      "807/807 [==============================] - 48s 59ms/step - loss: 5.7926 - accuracy: 0.1327\n",
      "Epoch 5/15\n",
      "807/807 [==============================] - 47s 59ms/step - loss: 5.6789 - accuracy: 0.1387\n",
      "Epoch 6/15\n",
      "807/807 [==============================] - 48s 59ms/step - loss: 5.5804 - accuracy: 0.1442\n",
      "Epoch 7/15\n",
      "807/807 [==============================] - 47s 58ms/step - loss: 5.4955 - accuracy: 0.1494\n",
      "Epoch 8/15\n",
      "807/807 [==============================] - 47s 58ms/step - loss: 5.4176 - accuracy: 0.1542\n",
      "Epoch 9/15\n",
      "807/807 [==============================] - 48s 60ms/step - loss: 5.3496 - accuracy: 0.1588\n",
      "Epoch 10/15\n",
      "807/807 [==============================] - 47s 59ms/step - loss: 5.2855 - accuracy: 0.1627\n",
      "Epoch 11/15\n",
      "807/807 [==============================] - 47s 58ms/step - loss: 5.2272 - accuracy: 0.1669\n",
      "Epoch 12/15\n",
      "807/807 [==============================] - 48s 60ms/step - loss: 5.1756 - accuracy: 0.1698\n",
      "Epoch 13/15\n",
      "807/807 [==============================] - 46s 57ms/step - loss: 5.1330 - accuracy: 0.1728\n",
      "Epoch 14/15\n",
      "807/807 [==============================] - 47s 59ms/step - loss: 5.0881 - accuracy: 0.1748\n",
      "Epoch 15/15\n",
      "807/807 [==============================] - 47s 58ms/step - loss: 5.0512 - accuracy: 0.1775\n",
      "testing words gen\n",
      "'this movie' -> 'this movie is fabulous again at least this movie in scenes which'\n",
      "'the film' -> 'the film was if he was i knew it off from this'\n",
      "'i think' -> 'i think are dynamic completely tries to doubt it came on up'\n",
      "'the story' -> 'the story up ultimately the very story and the curtain on a'\n",
      "\n",
      "text gen: SYMBOLS\n",
      "'the movie was' -> 'the movie was he was sure that i was v i gu'\n",
      "'i really liked' -> 'i really liked is showing soundtripping he f'\n",
      "'the acting is' -> 'the acting is about the good little film au'\n",
      "\n",
      "text gen: WORDS\n",
      "'the movie was' -> 'the movie was atrocious show makes him want to'\n",
      "'i really liked' -> 'i really liked it br br stardust is a'\n",
      "'the acting is' -> 'the acting is uneven looking and always indeed conspiracies'\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def prepare_char_level_data(texts, seq_length=5, max_texts=1000):\n",
    "    if len(texts) > max_texts:\n",
    "        texts = random.sample(texts, max_texts)\n",
    "\n",
    "    print(f\"texts: {len(texts)}\")\n",
    "\n",
    "    all_text = ' '.join(texts)\n",
    "    \n",
    "    if len(all_text) == 0:\n",
    "        return [], [], {}, {}, 0\n",
    "\n",
    "    chars = sorted(list(set(all_text)))\n",
    "    print(f\"unique symbols: {len(chars)}\")\n",
    "\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "\n",
    "    for text in texts:\n",
    "        for i in range(0, len(text) - seq_length):\n",
    "            seq = text[i:i + seq_length]\n",
    "            next_char = text[i + seq_length]\n",
    "            sequences.append([char_to_idx[char] for char in seq])\n",
    "            next_chars.append(char_to_idx[next_char])\n",
    "\n",
    "    print(f\"sequences count: {len(sequences)}\")\n",
    "    return sequences, next_chars, char_to_idx, idx_to_char, len(chars)\n",
    "\n",
    "def build_char_model(vocab_size, seq_length=5):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, 128, input_length=seq_length),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_char_text(model, seed_text, char_to_idx, idx_to_char, seq_length=5, num_chars=50):\n",
    "    generated = seed_text\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        seq = [char_to_idx.get(char, 0) for char in seed_text[-seq_length:]]\n",
    "        seq = tf.keras.preprocessing.sequence.pad_sequences([seq], maxlen=seq_length, padding='pre')\n",
    "\n",
    "        pred = model.predict(seq, verbose=0)[0]\n",
    "        next_idx = random.choices(range(len(pred)), weights=pred)[0]\n",
    "        next_char = idx_to_char.get(next_idx, ' ')\n",
    "\n",
    "        generated += next_char\n",
    "        seed_text = seed_text[1:] + next_char if len(seed_text) >= seq_length else seed_text + next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "def prepare_word_level_data(texts, seq_length=3, max_texts=1000):\n",
    "    if len(texts) > max_texts:\n",
    "        texts = random.sample(texts, max_texts)\n",
    "\n",
    "    print(f\"texts: {len(texts)}\")\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for i in range(seq_length, len(words)):\n",
    "            seq = words[i-seq_length:i]\n",
    "            sequences.append(' '.join(seq))\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        if len(seq) == seq_length:\n",
    "            X.append(seq)  # Вся последовательность\n",
    "            y.append(seq[-1])  # Последнее слово как целевое\n",
    "\n",
    "    # Теперь X содержит последовательности длины seq_length, нужно разделить на входы и цели\n",
    "    X_input = [seq[:-1] for seq in X]  # Все кроме последнего слова\n",
    "    y_target = [seq[-1] for seq in X]   # Только последнее слово\n",
    "\n",
    "    print(f\"word sequences: {len(X_input)}\")\n",
    "    print(f\"dictionary size: {len(tokenizer.word_index) + 1}\")\n",
    "    print(f\"input sequence length: {len(X_input[0]) if len(X_input) > 0 else 0}\")\n",
    "\n",
    "    return X_input, y_target, tokenizer, len(tokenizer.word_index) + 1\n",
    "\n",
    "def build_word_model(vocab_size, seq_length=2):  # Изменено на 2, т.к. входная длина = seq_length - 1\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, 256, input_length=seq_length),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_word_text(model, seed_text, tokenizer, seq_length=2, num_words=10, temperature=1.0):\n",
    "    generated = seed_text.split()\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        seed_words = generated[-seq_length:]  # Берем последние seq_length слов\n",
    "        token_list = tokenizer.texts_to_sequences([' '.join(seed_words)])[0]\n",
    "\n",
    "        if len(token_list) < seq_length:\n",
    "            token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], \n",
    "                                                                     maxlen=seq_length, \n",
    "                                                                     padding='pre')[0]\n",
    "        else:\n",
    "            token_list = token_list[:seq_length]\n",
    "\n",
    "        predictions = model.predict(np.array([token_list]), verbose=0)[0]\n",
    "\n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        next_idx = random.choices(range(len(predictions)), weights=predictions)[0]\n",
    "\n",
    "        next_word = \"\"\n",
    "        for word, idx in tokenizer.word_index.items():\n",
    "            if idx == next_idx:\n",
    "                next_word = word\n",
    "                break\n",
    "\n",
    "        if next_word:\n",
    "            generated.append(next_word)\n",
    "\n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Проверка GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU: {gpus[0]}\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "\n",
    "# Загрузка данных\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=15000)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "texts = []\n",
    "for i in range(min(10000, len(x_train))):\n",
    "    texts.append(decode_review(x_train[i]))\n",
    "\n",
    "print(f\"Загружено текстов: {len(texts)}\")\n",
    "\n",
    "# Обучение моделей\n",
    "print(\"\\n1. symbols gen\")\n",
    "char_sequences, next_chars, char_to_idx, idx_to_char, char_vocab_size = prepare_char_level_data(texts)\n",
    "\n",
    "if len(char_sequences) > 0:\n",
    "    X_char = np.array(char_sequences)\n",
    "    y_char = np.array(next_chars)\n",
    "\n",
    "    char_model = build_char_model(char_vocab_size)\n",
    "    print(\"training symbols model\")\n",
    "    char_model.fit(X_char, y_char, batch_size=256, epochs=15, verbose=1)\n",
    "\n",
    "    print(\"testing symbols gen\")\n",
    "    seed_texts = [\"the movie\", \"this film\", \"i think\", \"the story\"]\n",
    "    for seed in seed_texts:\n",
    "        if all(char in char_to_idx for char in seed):\n",
    "            generated = generate_char_text(char_model, seed, char_to_idx, idx_to_char)\n",
    "            print(f\"'{seed}' -> '{generated}'\")\n",
    "        else:\n",
    "            print(f\"symbols '{seed}' not found in dictionary\")\n",
    "\n",
    "# Демонстрация\n",
    "if 'char_model' in locals() and len(char_sequences) > 0:\n",
    "    print(\"\\ntext gen: SYMBOLS\")\n",
    "    demo_seeds = [\"the movie was\", \"i really liked\", \"the acting is\"]\n",
    "    for seed in demo_seeds:\n",
    "        if all(char in char_to_idx for char in seed):\n",
    "            result = generate_char_text(char_model, seed, char_to_idx, idx_to_char, num_chars=30)\n",
    "            print(f\"'{seed}' -> '{result}'\")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059b439d-09c3-4bb8-a11f-29eb8edfc89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IMPROVED WORD-LEVEL TEXT GENERATION ===\n",
      "Processing 2000 texts\n",
      "Target vocabulary: 3000 words\n",
      "Sequence length: 4\n",
      "After length filtering: 2000 texts\n",
      "Generated 454074 clean training sequences\n",
      "Actual vocabulary used: 11735 words (appearing ≥2 times)\n",
      "Training data shape: (454074, 4)\n",
      "Vocabulary size: 3000\n",
      "Sample sequences: [[ 465   75 1171   12]\n",
      " [  75 1171   12  105]\n",
      " [1171   12  105    1]]\n",
      "Sample targets: [105   1  17]\n",
      "Model summary:\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 4, 64)             192000    \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 3000)              99000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,152\n",
      "Trainable params: 400,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training word-level model...\n",
      "Epoch 1/50\n",
      " 378/6031 [>.............................] - ETA: 52s - loss: 6.0028 - accuracy: 0.1014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 173\u001b[0m\n\u001b[0;32m    156\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    157\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m    158\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    170\u001b[0m ]\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining word-level model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mword_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Уменьшил batch_size\u001b[39;49;00m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Увеличил эпохи\u001b[39;49;00m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Добавил перемешивание\u001b[39;49;00m\n\u001b[0;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Детальный анализ обучения\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== TRAINING ANALYSIS ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mE:\\soft\\anaconda3\\envs\\tensorflow310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_word_level_data(texts, seq_length=4, max_texts=2000, num_words=3000):\n",
    "    \"\"\"Улучшенная подготовка данных с фильтрацией и балансировкой\"\"\"\n",
    "    if len(texts) > max_texts:\n",
    "        texts = random.sample(texts, max_texts)\n",
    "\n",
    "    print(f\"Processing {len(texts)} texts\")\n",
    "    print(f\"Target vocabulary: {num_words} words\")\n",
    "    print(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "    # Фильтруем слишком короткие тексты\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        if len(words) >= seq_length + 2:  # Минимум seq_length + 2 слова\n",
    "            filtered_texts.append(text)\n",
    "    \n",
    "    texts = filtered_texts\n",
    "    print(f\"After length filtering: {len(texts)} texts\")\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=num_words, \n",
    "        oov_token=\"<OOV>\",\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # Базовые фильтры\n",
    "    )\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= seq_length + 1:\n",
    "            for i in range(seq_length, len(seq)):\n",
    "                # Проверяем, что все слова в последовательности ненулевые\n",
    "                if all(token != 0 for token in seq[i-seq_length:i+1]):\n",
    "                    X.append(seq[i-seq_length:i])\n",
    "                    y.append(seq[i])\n",
    "\n",
    "    print(f\"Generated {len(X)} clean training sequences\")\n",
    "    print(f\"Actual vocabulary used: {len([w for w in tokenizer.word_counts.items() if w[1] >= 2])} words (appearing ≥2 times)\")\n",
    "    \n",
    "    return X, y, tokenizer, min(num_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "def generate_word_text(model, seed_text, tokenizer, seq_length=4, num_words=8, temperature=1.0):\n",
    "    \"\"\"Улучшенная генерация с обработкой ошибок\"\"\"\n",
    "    generated = seed_text.split()\n",
    "    \n",
    "    # Убедимся, что начальное семя не слишком короткое\n",
    "    if len(generated) < seq_length:\n",
    "        # Дополним случайными словами из словаря если нужно\n",
    "        while len(generated) < seq_length:\n",
    "            random_word = random.choice(list(tokenizer.word_index.keys())[:100])  # Только частые слова\n",
    "            generated.insert(0, random_word)\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        try:\n",
    "            # Берем последние seq_length слов\n",
    "            seed_words = generated[-seq_length:]\n",
    "            \n",
    "            # Преобразуем в последовательность токенов\n",
    "            token_list = tokenizer.texts_to_sequences([' '.join(seed_words)])[0]\n",
    "            \n",
    "            # Дополняем до нужной длины\n",
    "            if len(token_list) < seq_length:\n",
    "                token_list = [0] * (seq_length - len(token_list)) + token_list\n",
    "            else:\n",
    "                token_list = token_list[-seq_length:]\n",
    "            \n",
    "            # Предсказание\n",
    "            predictions = model.predict(np.array([token_list]), verbose=0)[0]\n",
    "            \n",
    "            # Температурное преобразование\n",
    "            predictions = np.log(predictions + 1e-7) / temperature\n",
    "            exp_preds = np.exp(predictions)\n",
    "            predictions = exp_preds / np.sum(exp_preds)\n",
    "            \n",
    "            # Исключаем OOV токены из выбора\n",
    "            valid_indices = [i for i in range(len(predictions)) \n",
    "                           if i in tokenizer.index_word and i != 0]  # Исключаем padding и OOV\n",
    "            \n",
    "            if not valid_indices:\n",
    "                next_word = \"the\"  # Fallback слово\n",
    "            else:\n",
    "                valid_predictions = [predictions[i] for i in valid_indices]\n",
    "                valid_predictions = np.array(valid_predictions) / np.sum(valid_predictions)\n",
    "                \n",
    "                next_idx = random.choices(valid_indices, weights=valid_predictions)[0]\n",
    "                next_word = tokenizer.index_word[next_idx]\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in generation: {e}\")\n",
    "            generated.append(\"the\")  # Fallback\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "print(\"\\n=== IMPROVED WORD-LEVEL TEXT GENERATION ===\")\n",
    "\n",
    "# Улучшенная подготовка данных\n",
    "word_sequences, next_words, word_tokenizer, word_vocab_size = prepare_word_level_data(\n",
    "    texts, \n",
    "    seq_length=4,  # Уменьшил для лучшего обучения\n",
    "    max_texts=2000,  # Уменьшил количество текстов\n",
    "    num_words=3000   # Сильно уменьшил словарь\n",
    ")\n",
    "\n",
    "if len(word_sequences) > 0:\n",
    "    X_word = np.array(word_sequences)\n",
    "    y_word = np.array(next_words)\n",
    "    \n",
    "    print(f\"Training data shape: {X_word.shape}\")\n",
    "    print(f\"Vocabulary size: {word_vocab_size}\")\n",
    "    print(f\"Sample sequences: {X_word[:3]}\")\n",
    "    print(f\"Sample targets: {y_word[:3]}\")\n",
    "\n",
    "    # УПРОЩЕННАЯ модель для лучшего обучения\n",
    "    word_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            word_vocab_size, \n",
    "            64,  # Уменьшил размерность\n",
    "            input_length=4,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        \n",
    "        # Упростил архитектуру - убрал BatchNormalization и уменьшил слои\n",
    "        tf.keras.layers.LSTM(128, return_sequences=False, dropout=0.1),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        \n",
    "        tf.keras.layers.Dense(word_vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Увеличил learning rate и настроил оптимизатор\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.005,  # Увеличил в 5 раз\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "    )\n",
    "    \n",
    "    word_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Model summary:\")\n",
    "    word_model.summary()\n",
    "\n",
    "    # Улучшенные callback'и\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss', \n",
    "            patience=5,\n",
    "            min_delta=0.01,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=0.0001,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTraining word-level model...\")\n",
    "    history = word_model.fit(\n",
    "        X_word, \n",
    "        y_word, \n",
    "        batch_size=64,  # Уменьшил batch_size\n",
    "        epochs=50,       # Увеличил эпохи\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        validation_split=0.15,\n",
    "        shuffle=True     # Добавил перемешивание\n",
    "    )\n",
    "\n",
    "    # Детальный анализ обучения\n",
    "    print(\"\\n=== TRAINING ANALYSIS ===\")\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    print(f\"Final training accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Final training loss: {final_loss:.4f}\")\n",
    "    \n",
    "    if 'val_accuracy' in history.history:\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        print(f\"Final validation accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Final validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Проверка на тренировочных данных\n",
    "    print(\"\\n=== QUICK TRAINING SAMPLE CHECK ===\")\n",
    "    sample_indices = np.random.choice(len(X_word), min(5, len(X_word)), replace=False)\n",
    "    for idx in sample_indices:\n",
    "        sample_input = X_word[idx:idx+1]\n",
    "        sample_target = y_word[idx]\n",
    "        prediction = word_model.predict(sample_input, verbose=0)\n",
    "        pred_word_idx = np.argmax(prediction[0])\n",
    "        actual_word = word_tokenizer.index_word.get(sample_target, '<?>')\n",
    "        pred_word = word_tokenizer.index_word.get(pred_word_idx, '<?>')\n",
    "        print(f\"Input seq: {sample_input[0]} -> Actual: '{actual_word}', Pred: '{pred_word}'\")\n",
    "\n",
    "    # Тестирование генерации\n",
    "    print(\"\\n=== IMPROVED GENERATION TESTING ===\")\n",
    "    \n",
    "    # Функция для безопасной генерации\n",
    "    def safe_generate(seed, temperature=0.7, num_words=6):\n",
    "        try:\n",
    "            return generate_word_text(\n",
    "                word_model, \n",
    "                seed, \n",
    "                word_tokenizer, \n",
    "                seq_length=4, \n",
    "                num_words=num_words, \n",
    "                temperature=temperature\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Generation error: {e}\"\n",
    "\n",
    "    # Простые тестовые сиды\n",
    "    test_seeds = [\n",
    "        \"i like this\",\n",
    "        \"the movie is\",\n",
    "        \"this film\",\n",
    "        \"it was very\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSimple generation tests:\")\n",
    "    for seed in test_seeds:\n",
    "        result = safe_generate(seed, temperature=0.8, num_words=4)\n",
    "        print(f\"'{seed}' -> '{result}'\")\n",
    "\n",
    "    # Основная демонстрация\n",
    "    if final_accuracy > 0.3:  # Только если модель обучилась достаточно хорошо\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MAIN DEMONSTRATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        demo_seeds = [\n",
    "            \"this movie is\",\n",
    "            \"i really think\", \n",
    "            \"the story was\",\n",
    "            \"the acting is\"\n",
    "        ]\n",
    "        \n",
    "        for seed in demo_seeds:\n",
    "            print(f\"\\n--- {seed} ---\")\n",
    "            for temp in [0.5, 0.7, 1.0]:\n",
    "                result = safe_generate(seed, temperature=temp, num_words=6)\n",
    "                print(f\"Temp {temp}: {result}\")\n",
    "    else:\n",
    "        print(f\"\\nModel accuracy too low ({final_accuracy:.4f}) for meaningful generation.\")\n",
    "        print(\"Consider: increasing training data, decreasing vocabulary size, or simplifying model further.\")\n",
    "\n",
    "print('\\nWord generation process completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190966a-1725-4788-ac85-b2a71fbba437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89938492-700d-4545-a17d-027c1e5d48a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
